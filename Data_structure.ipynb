{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3542b2d6-acbb-4e7a-9991-94d6983374a5",
   "metadata": {},
   "source": [
    "Business Question\r\n",
    "Customer behaviour analysis\r\n",
    "Why customers stop using a product or service?\r\n",
    "What percentage of customers have churned in the past quarter?\r\n",
    "Predict/recommend the next order of a customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b093e43-cf64-44d3-8cb2-620a728af0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2398795</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473747</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2254736</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>431534</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
       "0   2539329        1    prior             1          2                  8   \n",
       "1   2398795        1    prior             2          3                  7   \n",
       "2    473747        1    prior             3          3                 12   \n",
       "3   2254736        1    prior             4          4                  7   \n",
       "4    431534        1    prior             5          4                 15   \n",
       "\n",
       "   days_since_prior_order  \n",
       "0                     NaN  \n",
       "1                    15.0  \n",
       "2                    21.0  \n",
       "3                    29.0  \n",
       "4                    28.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your local file\n",
    "file_path = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\orders.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "orders_data = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few rows to check\n",
    "orders_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128baec6-a116-4a9c-ba08-c808ef51c4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                       0\n",
       "user_id                        0\n",
       "eval_set                       0\n",
       "order_number                   0\n",
       "order_dow                      0\n",
       "order_hour_of_day              0\n",
       "days_since_prior_order    206209\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c1b183-63dd-45c1-aade-39af5fb9c194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3060951-4963-471d-9209-c2e2d0ff27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data['days_since_prior_order'] = orders_data['days_since_prior_order'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908302c4-cd89-4fbb-a781-28f3c888f75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                  0\n",
       "user_id                   0\n",
       "eval_set                  0\n",
       "order_number              0\n",
       "order_dow                 0\n",
       "order_hour_of_day         0\n",
       "days_since_prior_order    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6028037-1f5d-4d1b-8728-91cba6f6f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_data['days_since_prior_order'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e4a37a-0120-4eb2-92cf-80986a0717b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>33120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28985</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9327</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>45918</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>30035</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  product_id  add_to_cart_order  reordered\n",
       "0         2       33120                  1          1\n",
       "1         2       28985                  2          1\n",
       "2         2        9327                  3          0\n",
       "3         2       45918                  4          1\n",
       "4         2       30035                  5          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your local file\n",
    "file_path_order_products = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\order_products__prior.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "order_products__prior = pd.read_csv(file_path_order_products)\n",
    "\n",
    "# Print the first few rows to check\n",
    "order_products__prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee53c75-7a7a-4096-804d-a4b7c7c9fd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id             0\n",
       "product_id           0\n",
       "add_to_cart_order    0\n",
       "reordered            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_products__prior.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb225b7-1fc9-42e1-9327-9b027232e2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_products__prior.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46daf345-a7b8-490e-bfca-af930a6dcfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>aisle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>prepared soups salads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>specialty cheeses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>energy granola bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>instant foods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>marinades meat preparation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aisle_id                       aisle\n",
       "0         1       prepared soups salads\n",
       "1         2           specialty cheeses\n",
       "2         3         energy granola bars\n",
       "3         4               instant foods\n",
       "4         5  marinades meat preparation"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aisles\n",
    "\n",
    "# Define the path to your local file\n",
    "file_path_aisles = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\aisles.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "aisles = pd.read_csv(file_path_aisles)\n",
    "\n",
    "# Print the first few rows to check\n",
    "aisles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287ddafb-0d0c-42b0-ab7f-05613a247d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aisle_id    0\n",
       "aisle       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aisles.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babbbb15-5a85-4371-841e-8de4cee62b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aisles.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f4f8c8-95da-47ff-bc3b-62b85943833c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department_id</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>frozen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bakery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   department_id department\n",
       "0              1     frozen\n",
       "1              2      other\n",
       "2              3     bakery\n",
       "3              4    produce\n",
       "4              5    alcohol"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#departments\n",
    "\n",
    "# Define the path to your local file\n",
    "file_path_departments = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\departments.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "departments = pd.read_csv(file_path_departments)\n",
    "\n",
    "# Print the first few rows to check\n",
    "departments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f463e1dd-fc02-4dcf-b08a-fe6cdba20676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "department_id    0\n",
       "department       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departments.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "069f5e41-52cc-4cbb-a450-0fd090147826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departments.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ecf41a-e330-4ccd-a46f-2e9fa931d72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11109</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10246</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>49683</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>43633</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  product_id  add_to_cart_order  reordered\n",
       "0         1       49302                  1          1\n",
       "1         1       11109                  2          1\n",
       "2         1       10246                  3          0\n",
       "3         1       49683                  4          0\n",
       "4         1       43633                  5          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order_products__train\n",
    "\n",
    "# Define the path to your local file\n",
    "file_path_order_products_train = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\order_products__train.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "order_products_train = pd.read_csv(file_path_order_products_train)\n",
    "\n",
    "# Print the first few rows to check\n",
    "order_products_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d066130-58d6-428a-b2a2-91ea8ceb7ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id             0\n",
       "product_id           0\n",
       "add_to_cart_order    0\n",
       "reordered            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_products_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fbb5818-2c19-42a0-9ff6-819d0097ad3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_products_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bff9289-e556-4d43-ae8b-51588918d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chocolate Sandwich Cookies</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All-Seasons Salt</td>\n",
       "      <td>104</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Robust Golden Unsweetened Oolong Tea</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Smart Ones Classic Favorites Mini Rigatoni Wit...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Green Chile Anytime Sauce</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                       product_name  aisle_id  \\\n",
       "0           1                         Chocolate Sandwich Cookies        61   \n",
       "1           2                                   All-Seasons Salt       104   \n",
       "2           3               Robust Golden Unsweetened Oolong Tea        94   \n",
       "3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n",
       "4           5                          Green Chile Anytime Sauce         5   \n",
       "\n",
       "   department_id  \n",
       "0             19  \n",
       "1             13  \n",
       "2              7  \n",
       "3              1  \n",
       "4             13  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#products\n",
    "\n",
    "# Define the path to your local file\n",
    "file_path_products = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\products.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "products = pd.read_csv(file_path_products)\n",
    "\n",
    "# Print the first few rows to check\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba1c593-8a74-496f-bdb6-0c55d7d14445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id       0\n",
       "product_name     0\n",
       "aisle_id         0\n",
       "department_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66aae404-26e9-44bc-9976-3dad2840226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "703722bb-d127-4c48-b592-f702ab11d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inactive customers in the last 30 days: 152012\n",
      "Total number of unique customers: 206209\n",
      "Churn rate: 73.72%\n"
     ]
    }
   ],
   "source": [
    "# Convert 'days_since_prior_order' column to numeric (if needed)\n",
    "orders_data['days_since_prior_order'] = pd.to_numeric(orders_data['days_since_prior_order'], errors='coerce')\n",
    "\n",
    "# 1. Calculate the number of inactive customers in the last 30 days\n",
    "inactive_customers = (\n",
    "    orders_data.groupby('user_id')['days_since_prior_order']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'days_since_prior_order': 'max_days_since_prior'})\n",
    ")\n",
    "\n",
    "# Get customers inactive for 30 days or more\n",
    "inactive_customers_30_days = inactive_customers[\n",
    "    (inactive_customers['max_days_since_prior'] >= 30) | (inactive_customers['max_days_since_prior'].isna())\n",
    "]\n",
    "\n",
    "inactive_customers_30_days_count = inactive_customers_30_days['user_id'].nunique()\n",
    "print(f\"Number of inactive customers in the last 30 days: {inactive_customers_30_days_count}\")\n",
    "\n",
    "# 2. Calculate churn rate based on inactive customers in the last 30 days\n",
    "total_customers = orders_data['user_id'].nunique()\n",
    "print(f\"Total number of unique customers: {total_customers}\")\n",
    "\n",
    "# Calculate churn rate\n",
    "churn_rate = (inactive_customers_30_days_count / total_customers) * 100\n",
    "print(f\"Churn rate: {churn_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b76026f-a7d3-492a-bca9-8dd61e9ed252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.295749970925335e-29\n",
      "R-squared: 1.0\n",
      "        user_id  predicted_churn_probability_next_30_days\n",
      "24           25                                  1.000000\n",
      "32           33                                  0.633333\n",
      "40           41                                  0.733333\n",
      "43           44                                  1.000000\n",
      "65           66                                  0.766667\n",
      "...         ...                                       ...\n",
      "206159   206160                                  0.533333\n",
      "206162   206163                                  1.000000\n",
      "206187   206188                                  0.666667\n",
      "206195   206196                                  0.700000\n",
      "206202   206203                                  0.900000\n",
      "\n",
      "[18129 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the order_products_prior data from local path\n",
    "order_products_prior_path = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\order_products__prior.csv\"\n",
    "order_products_prior = pd.read_csv(order_products_prior_path)\n",
    "\n",
    "# Load the orders_data from local path (as done earlier)\n",
    "orders_data_path = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\orders.csv\"\n",
    "orders_data = pd.read_csv(orders_data_path)\n",
    "\n",
    "# Merge order_products_prior with orders_data to include user_id in order_products_prior\n",
    "order_products_prior = pd.merge(\n",
    "    order_products_prior, \n",
    "    orders_data[['order_id', 'user_id']], \n",
    "    on='order_id', \n",
    "    how='left', \n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "order_products_prior = order_products_prior.loc[:, ~order_products_prior.columns.str.endswith('_drop')]\n",
    "\n",
    "# Feature Engineering\n",
    "# Step 1: Calculate Recency and Frequency for each user from `orders_data`\n",
    "user_recency = orders_data.groupby('user_id')['days_since_prior_order'].min().reset_index()\n",
    "user_recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "user_frequency = orders_data.groupby('user_id')['order_number'].nunique().reset_index()\n",
    "user_frequency.rename(columns={'order_number': 'frequency'}, inplace=True)\n",
    "\n",
    "# Step 2: Calculate Monetary and Reordered Ratio from `order_products_prior`\n",
    "user_monetary = order_products_prior.groupby('user_id')['product_id'].count().reset_index()\n",
    "user_monetary.rename(columns={'product_id': 'monetary_value'}, inplace=True)\n",
    "\n",
    "user_reorder_ratio = order_products_prior.groupby('user_id')['reordered'].mean().reset_index()\n",
    "user_reorder_ratio.rename(columns={'reordered': 'reorder_ratio'}, inplace=True)\n",
    "\n",
    "# Average Order Size per User\n",
    "order_size = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "order_size.rename(columns={'product_id': 'avg_order_size'}, inplace=True)\n",
    "user_avg_order_size = orders_data[['order_id', 'user_id']].merge(order_size, on='order_id')\n",
    "user_avg_order_size = user_avg_order_size.groupby('user_id')['avg_order_size'].mean().reset_index()\n",
    "\n",
    "# Step 3: Combine all feature data\n",
    "features_data = user_recency.merge(user_frequency, on='user_id') \\\n",
    "                            .merge(user_monetary, on='user_id') \\\n",
    "                            .merge(user_reorder_ratio, on='user_id') \\\n",
    "                            .merge(user_avg_order_size, on='user_id')\n",
    "\n",
    "# Step 4: Define continuous target for churn probability based on recency\n",
    "features_data['churn_probability_next_30_days'] = features_data['recency'] / 30\n",
    "\n",
    "# Step 5: Prepare features and target variable for modeling\n",
    "features = ['recency', 'frequency', 'monetary_value', 'reorder_ratio', 'avg_order_size']\n",
    "X = features_data[features]\n",
    "y = features_data['churn_probability_next_30_days']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance using regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Predict churn probability for the next 30 days for all customers\n",
    "churn_probabilities = model.predict(X)\n",
    "\n",
    "# Add predicted probabilities to the dataset\n",
    "features_data['predicted_churn_probability_next_30_days'] = churn_probabilities\n",
    "\n",
    "# Identify high-risk customers (those with high predicted churn probability for the next 30 days)\n",
    "high_risk_customers = features_data[features_data['predicted_churn_probability_next_30_days'] > 0.5]\n",
    "\n",
    "# Print high-risk customers (optional)\n",
    "print(high_risk_customers[['user_id', 'predicted_churn_probability_next_30_days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "876f6bef-9f60-42cc-81d8-615d3c903834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0001891426754818443\n",
      "R-squared: 0.9963512064579912\n",
      "        user_id  predicted_churn_probability_next_30_days\n",
      "24           25                                  1.000000\n",
      "32           33                                  0.640000\n",
      "40           41                                  0.720000\n",
      "43           44                                  1.000000\n",
      "65           66                                  0.766667\n",
      "...         ...                                       ...\n",
      "206159   206160                                  0.513333\n",
      "206162   206163                                  1.000000\n",
      "206187   206188                                  0.673333\n",
      "206195   206196                                  0.693333\n",
      "206202   206203                                  0.880000\n",
      "\n",
      "[18248 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Merge order_products_prior with orders_data to include user_id in order_products_prior\n",
    "# Drop any extra `user_id` column if it exists in `order_products_prior`\n",
    "order_products_prior = pd.merge(\n",
    "    order_products_prior, \n",
    "    orders_data[['order_id', 'user_id']], \n",
    "    on='order_id', \n",
    "    how='left', \n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "order_products_prior = order_products_prior.loc[:, ~order_products_prior.columns.str.endswith('_drop')]\n",
    "\n",
    "# Feature Engineering\n",
    "# Step 1: Calculate Recency and Frequency for each user from `orders_data`\n",
    "user_recency = orders_data.groupby('user_id')['days_since_prior_order'].min().reset_index()\n",
    "user_recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "user_frequency = orders_data.groupby('user_id')['order_number'].nunique().reset_index()\n",
    "user_frequency.rename(columns={'order_number': 'frequency'}, inplace=True)\n",
    "\n",
    "# Step 2: Calculate Monetary and Reordered Ratio from `order_products_prior`\n",
    "user_monetary = order_products_prior.groupby('user_id')['product_id'].count().reset_index()\n",
    "user_monetary.rename(columns={'product_id': 'monetary_value'}, inplace=True)\n",
    "\n",
    "user_reorder_ratio = order_products_prior.groupby('user_id')['reordered'].mean().reset_index()\n",
    "user_reorder_ratio.rename(columns={'reordered': 'reorder_ratio'}, inplace=True)\n",
    "\n",
    "# Average Order Size per User\n",
    "order_size = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "order_size.rename(columns={'product_id': 'avg_order_size'}, inplace=True)\n",
    "user_avg_order_size = orders_data[['order_id', 'user_id']].merge(order_size, on='order_id')\n",
    "user_avg_order_size = user_avg_order_size.groupby('user_id')['avg_order_size'].mean().reset_index()\n",
    "\n",
    "# Step 3: Combine all feature data\n",
    "features_data = user_recency.merge(user_frequency, on='user_id') \\\n",
    "                            .merge(user_monetary, on='user_id') \\\n",
    "                            .merge(user_reorder_ratio, on='user_id') \\\n",
    "                            .merge(user_avg_order_size, on='user_id')\n",
    "\n",
    "# Step 4: Define continuous target for churn probability based on recency\n",
    "features_data['churn_probability_next_30_days'] = features_data['recency'] / 30\n",
    "\n",
    "# Step 5: Prepare features and target variable for modeling\n",
    "features = ['recency', 'frequency', 'monetary_value', 'reorder_ratio', 'avg_order_size']\n",
    "X = features_data[features]\n",
    "y = features_data['churn_probability_next_30_days']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the K-Nearest Neighbors Regressor model\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)  # Set the number of neighbors\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance using regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Predict churn probability for the next 30 days for all customers\n",
    "churn_probabilities = knn_model.predict(X)\n",
    "\n",
    "# Add predicted probabilities to the dataset\n",
    "features_data['predicted_churn_probability_next_30_days'] = churn_probabilities\n",
    "\n",
    "# Identify high-risk customers (those with high predicted churn probability for the next 30 days)\n",
    "high_risk_customers = features_data[features_data['predicted_churn_probability_next_30_days'] > 0.5]\n",
    "\n",
    "# Print high-risk customers (optional)\n",
    "print(high_risk_customers[['user_id', 'predicted_churn_probability_next_30_days']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae090848-a07a-4d44-b160-07a635ff585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_id', 'product_id', 'add_to_cart_order', 'reordered',\n",
      "       'user_id_prior', 'user_id_order'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m user_frequency\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_number\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Step 2: Calculate Monetary and Reordered Ratio from `order_products__prior`\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m user_monetary \u001b[38;5;241m=\u001b[39m order_products_prior\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     19\u001b[0m user_monetary\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonetary_value\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m user_reorder_ratio \u001b[38;5;241m=\u001b[39m order_products_prior\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreordered\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8250\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8253\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8254\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   8255\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   8256\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   8257\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   8258\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   8259\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   8260\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   8261\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   8262\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m    932\u001b[0m         obj,\n\u001b[0;32m    933\u001b[0m         keys,\n\u001b[0;32m    934\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    935\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    936\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    937\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m    938\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m    939\u001b[0m     )\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_id'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Merge order_products__prior with orders_data to include user_id in order_products__prior\n",
    "order_products_prior = pd.merge(order_products_prior, orders_data[['order_id', 'user_id']], on='order_id', how='left', suffixes=('_prior', '_order'))\n",
    "\n",
    "# Check the columns after merge\n",
    "print(order_products_prior.columns)\n",
    "\n",
    "# Feature Engineering\n",
    "# Step 1: Calculate Recency and Frequency for each user from `orders_data`\n",
    "user_recency = orders_data.groupby('user_id')['days_since_prior_order'].min().reset_index()\n",
    "user_recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "user_frequency = orders_data.groupby('user_id')['order_number'].nunique().reset_index()\n",
    "user_frequency.rename(columns={'order_number': 'frequency'}, inplace=True)\n",
    "\n",
    "# Step 2: Calculate Monetary and Reordered Ratio from `order_products__prior`\n",
    "user_monetary = order_products_prior.groupby('user_id')['product_id'].count().reset_index()\n",
    "user_monetary.rename(columns={'product_id': 'monetary_value'}, inplace=True)\n",
    "\n",
    "user_reorder_ratio = order_products_prior.groupby('user_id')['reordered'].mean().reset_index()\n",
    "user_reorder_ratio.rename(columns={'reordered': 'reorder_ratio'}, inplace=True)\n",
    "\n",
    "# Average Order Size per User\n",
    "order_size = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "order_size.rename(columns={'product_id': 'avg_order_size'}, inplace=True)\n",
    "user_avg_order_size = orders_data[['order_id', 'user_id']].merge(order_size, on='order_id')\n",
    "user_avg_order_size = user_avg_order_size.groupby('user_id')['avg_order_size'].mean().reset_index()\n",
    "\n",
    "# Step 3: Combine all feature data\n",
    "features_data = user_recency.merge(user_frequency, on='user_id') \\\n",
    "                            .merge(user_monetary, on='user_id') \\\n",
    "                            .merge(user_reorder_ratio, on='user_id') \\\n",
    "                            .merge(user_avg_order_size, on='user_id')\n",
    "\n",
    "# Step 4: Define churn target for the next 30 days based on recency\n",
    "features_data['churn_next_30_days'] = (features_data['recency'] > 30).astype(int)\n",
    "\n",
    "# Step 5: Prepare features and target variable for modeling\n",
    "features = ['recency', 'frequency', 'monetary_value', 'reorder_ratio', 'avg_order_size']\n",
    "X = features_data[features]\n",
    "y = features_data['churn_next_30_days']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate model performance\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "\n",
    "# Step 9: Predict churn for the next 30 days for all customers\n",
    "churn_probabilities = model.predict(X)\n",
    "\n",
    "# Add predicted probabilities to the dataset\n",
    "features_data['churn_probability_next_30_days'] = churn_probabilities\n",
    "\n",
    "# Step 10: Identify high-risk customers (those with high churn probability for the next 30 days)\n",
    "high_risk_customers = features_data[features_data['churn_probability_next_30_days'] > 0.5]\n",
    "\n",
    "# Print high-risk customers (optional)\n",
    "print(high_risk_customers[['user_id', 'churn_probability_next_30_days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1153d157-b4b4-4ca2-8368-f0913775fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_id', 'product_id', 'add_to_cart_order', 'reordered',\n",
      "       'user_id_prior', 'user_id_order_x', 'user_id_order_y'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user_id_order'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m user_frequency\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_number\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Step 5: Calculate Monetary and Reordered Ratio from `order_products_prior`\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m user_monetary \u001b[38;5;241m=\u001b[39m order_products_prior\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_order\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     31\u001b[0m user_monetary\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonetary_value\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m user_reorder_ratio \u001b[38;5;241m=\u001b[39m order_products_prior\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_order\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreordered\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8250\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8253\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8254\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   8255\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   8256\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   8257\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   8258\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   8259\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   8260\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   8261\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   8262\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m    932\u001b[0m         obj,\n\u001b[0;32m    933\u001b[0m         keys,\n\u001b[0;32m    934\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    935\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    936\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    937\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m    938\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m    939\u001b[0m     )\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_id_order'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Rename user_id in orders_data to avoid conflict\n",
    "orders_data = orders_data.rename(columns={'user_id': 'user_id_order'})\n",
    "\n",
    "# Step 2: Merge order_products_prior with orders_data to include user_id in order_products_prior\n",
    "order_products_prior = pd.merge(\n",
    "    order_products_prior, \n",
    "    orders_data[['order_id', 'user_id_order']],  # Use the renamed column\n",
    "    on='order_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 3: Check the columns after merge\n",
    "print(order_products_prior.columns)\n",
    "\n",
    "# Feature Engineering\n",
    "# Step 4: Calculate Recency and Frequency for each user from `orders_data`\n",
    "user_recency = orders_data.groupby('user_id_order')['days_since_prior_order'].min().reset_index()\n",
    "user_recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "user_frequency = orders_data.groupby('user_id_order')['order_number'].nunique().reset_index()\n",
    "user_frequency.rename(columns={'order_number': 'frequency'}, inplace=True)\n",
    "\n",
    "# Step 5: Calculate Monetary and Reordered Ratio from `order_products_prior`\n",
    "user_monetary = order_products_prior.groupby('user_id_order')['product_id'].count().reset_index()\n",
    "user_monetary.rename(columns={'product_id': 'monetary_value'}, inplace=True)\n",
    "\n",
    "user_reorder_ratio = order_products_prior.groupby('user_id_order')['reordered'].mean().reset_index()\n",
    "user_reorder_ratio.rename(columns={'reordered': 'reorder_ratio'}, inplace=True)\n",
    "\n",
    "# Step 6: Average Order Size per User\n",
    "order_size = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "order_size.rename(columns={'product_id': 'avg_order_size'}, inplace=True)\n",
    "user_avg_order_size = orders_data[['order_id', 'user_id_order']].merge(order_size, on='order_id')\n",
    "user_avg_order_size = user_avg_order_size.groupby('user_id_order')['avg_order_size'].mean().reset_index()\n",
    "\n",
    "# Step 7: Combine all feature data\n",
    "features_data = user_recency.merge(user_frequency, on='user_id_order') \\\n",
    "                            .merge(user_monetary, on='user_id_order') \\\n",
    "                            .merge(user_reorder_ratio, on='user_id_order') \\\n",
    "                            .merge(user_avg_order_size, on='user_id_order')\n",
    "\n",
    "# Step 8: Define churn target for the next 30 days based on recency\n",
    "features_data['churn_next_30_days'] = (features_data['recency'] > 30).astype(int)\n",
    "\n",
    "# Step 9: Prepare features and target variable for modeling\n",
    "features = ['recency', 'frequency', 'monetary_value', 'reorder_ratio', 'avg_order_size']\n",
    "X = features_data[features]\n",
    "y = features_data['churn_next_30_days']\n",
    "\n",
    "# Step 10: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 11: Train the AdaBoost model\n",
    "base_estimator = DecisionTreeClassifier(max_depth=3) \n",
    "model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 12: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 13: Evaluate model performance\n",
    "# Print classification report (Precision, Recall, F1-score)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 14: Predict churn for the next 30 days for all customers\n",
    "churn_probabilities = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Step 15: Add predicted probabilities to the dataset\n",
    "features_data['churn_probability_next_30_days'] = churn_probabilities\n",
    "\n",
    "# Step 16: Identify high-risk customers (those with high churn probability for the next 30 days)\n",
    "high_risk_customers = features_data[features_data['churn_probability_next_30_days'] > 0.5]\n",
    "\n",
    "# Step 17: Print high-risk customers (optional)\n",
    "print(high_risk_customers[['user_id_order', 'churn_probability_next_30_days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de8e3dbe-5fa3-4cde-a722-659da6da7424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing: \n",
      "churn_next_30_days\n",
      "0    206209\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'n_samples' parameter of resample must be an int in the range [1, inf) or None. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m minority \u001b[38;5;241m=\u001b[39m train_data[train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Churn = 1\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Downsample the majority class\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m majority_downsampled \u001b[38;5;241m=\u001b[39m resample(majority, \n\u001b[0;32m     71\u001b[0m                                  replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,        \u001b[38;5;66;03m# Sample without replacement\u001b[39;00m\n\u001b[0;32m     72\u001b[0m                                  n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(minority),  \u001b[38;5;66;03m# Match minority class size\u001b[39;00m\n\u001b[0;32m     73\u001b[0m                                  random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)      \u001b[38;5;66;03m# For reproducibility\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Combine back into a single balanced dataset\u001b[39;00m\n\u001b[0;32m     76\u001b[0m train_data_balanced \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([majority_downsampled, minority])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:201\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    199\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 201\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    202\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'n_samples' parameter of resample must be an int in the range [1, inf) or None. Got 0 instead."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Rename user_id in orders_data to avoid conflict\n",
    "orders_data = orders_data.rename(columns={'user_id': 'user_id_order'})\n",
    "\n",
    "# Ensure there is no 'user_id_order' in order_products_prior before merging\n",
    "if 'user_id_order' in order_products_prior.columns:\n",
    "    order_products_prior = order_products_prior.drop(columns=['user_id_order'])\n",
    "\n",
    "# Merge order_products_prior with orders_data to include user_id in order_products_prior\n",
    "order_products_prior = pd.merge(\n",
    "    order_products_prior, \n",
    "    orders_data[['order_id', 'user_id_order']],  # Use the renamed column\n",
    "    on='order_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Feature Engineering\n",
    "user_recency = orders_data.groupby('user_id_order')['days_since_prior_order'].min().reset_index()\n",
    "user_recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "user_frequency = orders_data.groupby('user_id_order')['order_number'].nunique().reset_index()\n",
    "user_frequency.rename(columns={'order_number': 'frequency'}, inplace=True)\n",
    "\n",
    "user_monetary = order_products_prior.groupby('user_id_order')['product_id'].count().reset_index()\n",
    "user_monetary.rename(columns={'product_id': 'monetary_value'}, inplace=True)\n",
    "\n",
    "user_reorder_ratio = order_products_prior.groupby('user_id_order')['reordered'].mean().reset_index()\n",
    "user_reorder_ratio.rename(columns={'reordered': 'reorder_ratio'}, inplace=True)\n",
    "\n",
    "order_size = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "order_size.rename(columns={'product_id': 'avg_order_size'}, inplace=True)\n",
    "\n",
    "user_avg_order_size = orders_data[['order_id', 'user_id_order']].merge(order_size, on='order_id')\n",
    "user_avg_order_size = user_avg_order_size.groupby('user_id_order')['avg_order_size'].mean().reset_index()\n",
    "\n",
    "# Merging all feature data into a single DataFrame\n",
    "features_data = user_recency.merge(user_frequency, on='user_id_order') \\\n",
    "                            .merge(user_monetary, on='user_id_order') \\\n",
    "                            .merge(user_reorder_ratio, on='user_id_order') \\\n",
    "                            .merge(user_avg_order_size, on='user_id_order')\n",
    "\n",
    "# Create the target variable 'churn_next_30_days' based on recency\n",
    "features_data['churn_next_30_days'] = (features_data['recency'] > 30).astype(int)\n",
    "\n",
    "# Feature columns and target variable\n",
    "features = ['recency', 'frequency', 'monetary_value', 'reorder_ratio', 'avg_order_size']\n",
    "X = features_data[features]\n",
    "y = features_data['churn_next_30_days']\n",
    "\n",
    "# Check class distribution before balancing\n",
    "print(f\"Class distribution before balancing: \\n{y.value_counts()}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Undersampling (manual)\n",
    "# Combine features and target into a single DataFrame for easy manipulation\n",
    "train_data = X_train.copy()\n",
    "train_data['target'] = y_train\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority = train_data[train_data['target'] == 0]  # Churn = 0\n",
    "minority = train_data[train_data['target'] == 1]  # Churn = 1\n",
    "\n",
    "# Downsample the majority class\n",
    "majority_downsampled = resample(majority, \n",
    "                                 replace=False,        # Sample without replacement\n",
    "                                 n_samples=len(minority),  # Match minority class size\n",
    "                                 random_state=42)      # For reproducibility\n",
    "\n",
    "# Combine back into a single balanced dataset\n",
    "train_data_balanced = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# Separate back into features (X) and target (y)\n",
    "X_train_balanced = train_data_balanced[features]\n",
    "y_train_balanced = train_data_balanced['target']\n",
    "\n",
    "# Check the new class distribution after undersampling\n",
    "print(f\"Class distribution after undersampling: \\n{y_train_balanced.value_counts()}\")\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict churn for the next 30 days for all customers\n",
    "churn_probabilities = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Add predicted probabilities to the dataset\n",
    "features_data['churn_probability_next_30_days'] = churn_probabilities\n",
    "\n",
    "# Identify high-risk customers (those with high churn probability for the next 30 days)\n",
    "high_risk_customers = features_data[features_data['churn_probability_next_30_days'] > 0.5]\n",
    "\n",
    "# Print high-risk customers (optional)\n",
    "print(high_risk_customers[['user_id_order', 'churn_probability_next_30_days']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd78688-1439-499b-aa58-bd290b987d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the order_products_prior data from local path\n",
    "order_products_prior_path = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\order_products__prior.csv\"\n",
    "order_products_prior = pd.read_csv(order_products_prior_path)\n",
    "\n",
    "# Load the orders_data from local path (as done earlier)\n",
    "orders_data_path = r\"C:\\Users\\HP\\Desktop\\Programming\\instacart-customer-churn-analyses\\orders.csv\"\n",
    "orders_data = pd.read_csv(orders_data_path)\n",
    "\n",
    "# Merge order_products_prior with orders_data to include user_id in order_products_prior\n",
    "order_products_prior = pd.merge(\n",
    "    order_products_prior, \n",
    "    orders_data[['order_id', 'user_id']], \n",
    "    on='order_id', \n",
    "    how='left', \n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "order_products_prior = order_products_prior.loc[:, ~order_products_prior.columns.str.endswith('_drop')]\n",
    "\n",
    "# Feature Engineering\n",
    "# Step 1: Calculate Recency and Frequency for each user from `orders_data`\n",
    "user_recency = orders_data.groupby('user_id')['days_since_prior_order'].min().reset_index()\n",
    "user_recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "user_frequency = orders_data.groupby('user_id')['order_number'].nunique().reset_index()\n",
    "user_frequency.rename(columns={'order_number': 'frequency'}, inplace=True)\n",
    "\n",
    "# Step 2: Calculate Monetary and Reordered Ratio from `order_products_prior`\n",
    "user_monetary = order_products_prior.groupby('user_id')['product_id'].count().reset_index()\n",
    "user_monetary.rename(columns={'product_id': 'monetary_value'}, inplace=True)\n",
    "\n",
    "user_reorder_ratio = order_products_prior.groupby('user_id')['reordered'].mean().reset_index()\n",
    "user_reorder_ratio.rename(columns={'reordered': 'reorder_ratio'}, inplace=True)\n",
    "\n",
    "# Average Order Size per User\n",
    "order_size = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "order_size.rename(columns={'product_id': 'avg_order_size'}, inplace=True)\n",
    "user_avg_order_size = orders_data[['order_id', 'user_id']].merge(order_size, on='order_id')\n",
    "user_avg_order_size = user_avg_order_size.groupby('user_id')['avg_order_size'].mean().reset_index()\n",
    "\n",
    "# Step 3: Combine all feature data\n",
    "features_data = user_recency.merge(user_frequency, on='user_id') \\\n",
    "                            .merge(user_monetary, on='user_id') \\\n",
    "                            .merge(user_reorder_ratio, on='user_id') \\\n",
    "                            .merge(user_avg_order_size, on='user_id')\n",
    "\n",
    "# Step 4: Define continuous target for churn probability based on recency\n",
    "features_data['churn_probability_next_30_days'] = features_data['recency'] / 30\n",
    "\n",
    "# Step 5: Prepare features and target variable for modeling\n",
    "features = ['recency', 'frequency', 'monetary_value', 'reorder_ratio', 'avg_order_size']\n",
    "X = features_data[features]\n",
    "y = features_data['churn_probability_next_30_days']\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance using regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Predict churn probability for the next 30 days for all customers\n",
    "churn_probabilities = model.predict(X_scaled)\n",
    "\n",
    "# Add predicted probabilities to the dataset\n",
    "features_data['predicted_churn_probability_next_30_days'] = churn_probabilities\n",
    "\n",
    "# Identify high-risk customers (those with high predicted churn probability for the next 30 days)\n",
    "high_risk_customers = features_data[features_data['predicted_churn_probability_next_30_days'] > 0.5]\n",
    "\n",
    "# Print high-risk customers (optional)\n",
    "print(high_risk_customers[['user_id', 'predicted_churn_probability_next_30_days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001d26b-34c9-48e5-ab6d-491a7a08e30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bf25d-080d-4c6e-bd9f-5c8160b17e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d162720-d5eb-4f6b-9ff1-47e10983d7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
